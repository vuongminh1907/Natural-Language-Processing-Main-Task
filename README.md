# ğŸŒ Natural Language Processing Main Task


This repository focuses on various NLP tasks with support for customizable training configurations.


## ğŸš€ Run Training

  You can start training your models using two methods:

1. **Direct Command Line Arguments**: 
  
2. **Using a Configuration File**: 

  Read through [config.md](https://github.com/vuongminh1907/Natural-Language-Processing-Main-Task/blob/main/docs/config.md) for instruction.

## ğŸ”¤ Tokenizer Training
In this section, you'll learn how to train a tokenizer from maybe one language to another language.
### ğŸ“¦ Dataset Preparation
Example format of `corpus.txt`:
```
TÃ´i yÃªu Viá»‡t Nam.
Cho tÃ´i má»™t like nhÃ©, yÃªu.
ÄÃ¢y lÃ  thá»­ nghiá»‡m NLP.
```
### ğŸƒâ€â™‚ï¸ Run Training
To train a tokenizer:
```
python train_tokenizer.py
```
## ğŸ§‘â€ğŸ’» Text Classification
Text Classification is a fundamental NLP task where the goal is to categorize text into predefined labels.


### ğŸ“¦ Dataset Preparation

  Ensure your dataset follows the `DatasetDict` format with the following features:
  ```
  ["attention_mask", "input_ids", "labels", "token_type_ids"]
  ```

To create the dataset:
- Save a `.txt` file with this format:
```
idx,sentence1,sentence2,label
1,"Example sentence 1","Example sentence 2",1
```
- Alternatively, you can use the **Hugging Face Datasets** library to load and process datasets directly.

### ğŸƒâ€â™‚ï¸ Run Training
To train the text classification model:
```
python train_text_classify.py
```
### ğŸ” Inference
```
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "bert-base-uncased"   #Replacing with your training model
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I love coffe", "I hate coffe"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```

## ğŸ“ Masked Natural Language Modeling
Masked Language Modeling involves predicting missing words in a sentence, allowing models to learn the context and relationships between words.

### ğŸ“¦ Dataset Preparation

  Ensure your dataset follows the `DatasetDict` format with the following features:
  ```
  
  ```

To create the dataset:
- Save a `.txt` file with this format:
```
idx,sentence
1,"Example sentence 1"
```
- Alternatively, you can use the **Hugging Face Datasets** library to load and process datasets directly.

### ğŸƒâ€â™‚ï¸ Run Training
To train the masked language model:
```
python train_masked_NL.py
```
### ğŸ” Inference
```
from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"  #Replacing with your model path 
)
text = "I love the [MASK]"
preds = mask_filler(text)
for pred in preds:
    print(f">>> {pred['sequence']}")
```

## ğŸŒ Translation
This section covers how to train a translation model that converts text from one language to another.

### ğŸ“¦ Dataset Preparation

  Ensure your dataset follows the `DatasetDict` format with the following features:
  ```
  DatasetDict({
            train: Dataset({
                features: ['id', 'translation'],
                num_rows: 210173
            })
        })
  ```

To create the dataset:
- Save a `translation_dataset.txt` file with this format:
```
idx,en,vie
1,"hello","Xin chÃ o"
```
- Alternatively, you can use the **Hugging Face Datasets** library to load and process datasets directly.

### ğŸƒâ€â™‚ï¸ Run Training
To train the translation model:
```
python train_trans.py
```
### ğŸ” Inference
```
python infer_trans.py
```

## ğŸ“ Summarization
This section covers how to train a summarization model that converts long text into a concise summary.

### ğŸ“¦ Dataset Preparation

  Ensure your dataset follows the `DatasetDict` format with the following features:
  ```
  DatasetDict({
            train: Dataset({
                features: ['Unnamed: 0', 'Document', 'Summary'],
                num_rows: ....
            })
        })
  ```

To create the dataset:
- Save a `summarization_dataset.txt` file with this format:
```
idx,Document,Summary
1,"SÃ¡ng nay, táº¡i HÃ  Ná»™i, má»™t cuá»™c há»p quan trá»ng Ä‘Ã£ diá»…n ra giá»¯a cÃ¡c nhÃ  lÃ£nh Ä‘áº¡o doanh nghiá»‡p lá»›n trong nÆ°á»›c. Cuá»™c há»p táº­p trung vÃ o cÃ¡c váº¥n Ä‘á» phÃ¡t triá»ƒn kinh táº¿ sau Ä‘áº¡i dá»‹ch vÃ  nhá»¯ng thÃ¡ch thá»©c mÃ  cÃ¡c doanh nghiá»‡p pháº£i Ä‘á»‘i máº·t. CÃ¡c Ä‘áº¡i biá»ƒu Ä‘Ã£ tháº£o luáº­n vá» viá»‡c tÄƒng cÆ°á»ng há»£p tÃ¡c cÃ´ng tÆ°, Ä‘á»•i má»›i cÃ´ng nghá»‡, vÃ  cáº£i thiá»‡n nÄƒng suáº¥t lao Ä‘á»™ng Ä‘á»ƒ thÃºc Ä‘áº©y tÄƒng trÆ°á»Ÿng kinh táº¿. CÃ¡c chuyÃªn gia kinh táº¿ cÅ©ng nháº¥n máº¡nh táº§m quan trá»ng cá»§a viá»‡c Ä‘Ã o táº¡o nguá»“n nhÃ¢n lá»±c cháº¥t lÆ°á»£ng cao vÃ  Ã¡p dá»¥ng cÃ¡c biá»‡n phÃ¡p há»— trá»£ doanh nghiá»‡p vÆ°á»£t qua khÃ³ khÄƒn.","Cuá»™c há»p táº¡i HÃ  Ná»™i bÃ n vá» phÃ¡t triá»ƒn kinh táº¿ sau Ä‘áº¡i dá»‹ch vÃ  tháº£o luáº­n cÃ¡c biá»‡n phÃ¡p há»— trá»£ doanh nghiá»‡p."

```
- Alternatively, you can use the **Hugging Face Datasets** library to load and process datasets directly.

### ğŸƒâ€â™‚ï¸ Run Training
To train the translation model:
```
python train_summarization.py
```
### ğŸ” Inference
```
python infer_sum.py
```


## ğŸ› ï¸ Additional Resources

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Hugging Face Datasets](https://huggingface.co/docs/datasets)
- [PyTorch](https://pytorch.org/)



